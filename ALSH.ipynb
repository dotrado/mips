{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "L2AlshTester l2 TEST:\n",
      "L\tk\tAcc\tData_ratio\tALSH time\tLinear time\n",
      "2\t1\t0.55\t0.99\t\t12.7495119572\t\t0.251654863358\n",
      "4\t1\t0.55\t1.0\t\t13.1768319607\t\t0.251654863358\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7baddf425cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0mdatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mlsh_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbours\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-7baddf425cea>\u001b[0m in \u001b[0;36mlsh_test\u001b[0;34m(datas, queries, rand_num, num_neighbours, mips)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0;34m'l_vec'\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             }\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0mtester\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \"\"\"\n",
      "\u001b[0;32m<ipython-input-17-7baddf425cea>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, type, k_vec, l_vec)\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhits\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mext_queries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact_hits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                     \u001b[0mlsh_hits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlsh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_neighbours\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m                     \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsh_hits\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0malsh_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-7baddf425cea>\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, q, metric, max_results)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_queries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# rerank candidates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mcandidates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-7baddf425cea>\u001b[0m in \u001b[0;36mdistance\u001b[0;34m(u, v)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# print \"L2Lsh.distance()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mux\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLshWrapper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-7baddf425cea>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m((ux, vx))\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# print \"L2Lsh.distance()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mux\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLshWrapper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import time\n",
    "\n",
    "def g_ext_norm(vec, m):      #L2-ALSH\n",
    "    l2norm_square = np.dot(vec, vec)\n",
    "    return [l2norm_square**(i+1) for i in xrange(m)]\n",
    "\n",
    "# get max norm for two-dimension list\n",
    "def g_max_norm(datas):\n",
    "    norm_list = [math.sqrt(np.dot(dd, dd)) for dd in datas]\n",
    "    return max(norm_list)\n",
    "\n",
    "# datas transformation. S(xi) = (U / M) * xi\n",
    "def g_transformation(datas):\n",
    "    # U < 1  ||xi||2 <= U <= 1. recommend for 0.83\n",
    "    U = 0.83\n",
    "    #U = 0.75\n",
    "    max_norm = g_max_norm(datas)\n",
    "    ratio = float(U / max_norm)\n",
    "    return ratio, max_norm, [[ratio * dx for dx in dd] for dd in datas]\n",
    "\n",
    "# normalization for each query\n",
    "def g_normalization(queries):\n",
    "    U = 0.83\n",
    "    #U = 0.75\n",
    "    norm_queries = []\n",
    "    for qv in queries:\n",
    "        norm = math.sqrt(np.dot(qv, qv))\n",
    "        ratio = float(U / norm)\n",
    "        norm_queries.append([ratio * qx for qx in qv])\n",
    "    return norm_queries\n",
    "\n",
    "class Hash:\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def hash(self, vec):\n",
    "        print \"Hash.hash()\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def distance(u, v):\n",
    "        print \"Hash.distance()\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def combine(hashes):\n",
    "        return str(hashes)\n",
    "\n",
    "class L2Lsh(Hash):\n",
    "    def __init__(self, r, d):\n",
    "        self.r, self.d = r, d\n",
    "        self.b = random.uniform(0, self.r)      # 0 < b < r\n",
    "        self.Data = [random.gauss(0, 1) for i in xrange(self.d)]\n",
    "\n",
    "    def hash(self, vec):\t# hash family\n",
    "        # use str() as a naive way of forming a single value\n",
    "        return int((np.dot(vec, self.Data) + self.b) / self.r)\n",
    "\n",
    "    # Euclidean Distance\n",
    "    @staticmethod\n",
    "    def distance(u, v):\n",
    "        # print \"L2Lsh.distance()\"\n",
    "        return sum((ux - vx)**2 for ux, vx in zip(u, v))**0.5\n",
    "\n",
    "class LshWrapper:\n",
    "    'LSH Wrapper'\n",
    "\n",
    "    # lsh_type: lsh hash func type: in 'l2\n",
    "    # r: random float data\n",
    "    # d: data vector size\n",
    "    # k: number of hash func for each hashtable. default 2\n",
    "    # L: number of hash tables for each hash type: default 2\n",
    "    def __init__(self, lsh_type, d, r = 1.0, k = 2, L = 2):\n",
    "        self.type = lsh_type\n",
    "        self.d, self.r, self.k, self.L, self.hash_tables = d, r, k, 0, []\n",
    "        self.resize(L)\n",
    "\n",
    "    def __get_hash_class__(self):\n",
    "        return L2Lsh\n",
    "        # return CosineLsh\n",
    "\n",
    "    def create_hash_table(self):\n",
    "        return L2Lsh(self.r, self.d)\n",
    "        # return CosineLsh(self.d)\n",
    "\n",
    "    def resize(self, L):\n",
    "        # shrink the number of hash tables to be used\n",
    "        if L < self.L:\n",
    "            self.hash_tables = self.hash_tables[:L]\n",
    "        else:\n",
    "            # initialise a new hash table for each hash function\n",
    "            hash_funcs = [[self.create_hash_table() for h in xrange(self.k)] for l in xrange(self.L, L)]\n",
    "            self.hash_tables.extend([(g, defaultdict(lambda:[])) for g in hash_funcs])\n",
    "        self.L = L\n",
    "\n",
    "    def hash(self, ht, data):\n",
    "        #  used for combine\n",
    "        return self.__get_hash_class__().combine([h.hash(data) for h in ht])\n",
    "\n",
    "    def index(self, datas):\n",
    "        # index the supplied datas\n",
    "        self.datas = datas\n",
    "        for ht, ct in self.hash_tables:\n",
    "            for ix, p in enumerate(self.datas):\n",
    "                ct[self.hash(ht, p)].append(ix)\n",
    "        # reset stats\n",
    "        self.tot_touched = 0\n",
    "        self.num_queries = 0\n",
    "\n",
    "    def query(self, q, metric, max_results = 1):\n",
    "        \"\"\"\n",
    "        triple_l = 3 * self.L\n",
    "        if max_results > triple_l:\n",
    "            max_results = triple_l\n",
    "        elif\n",
    "        \"\"\"\n",
    "        if 0 == max_results:\n",
    "            max_results = 1\n",
    "        # find the max_results closest indexed datas to q according to the supplied metric\n",
    "        candidates = set()\n",
    "        for ht, ct in self.hash_tables:\n",
    "            matches = ct.get(self.hash(ht, q), [])\n",
    "            candidates.update(matches)\n",
    "        # update stats\n",
    "        self.tot_touched += len(candidates)\n",
    "        self.num_queries += 1\n",
    "        # rerank candidates\n",
    "        candidates = [(ix, metric(q, self.datas[ix])) for ix in candidates]\n",
    "        candidates.sort(key=itemgetter(1))\n",
    "        return candidates[:max_results]\n",
    "\n",
    "    def get_avg_touched(self):\n",
    "        # mean number of candidates inspected per query\n",
    "        return self.tot_touched / self.num_queries\n",
    "\n",
    "    def distance(self, u, v):\n",
    "        return __get_hash_class__.distance(u, v)\n",
    "\n",
    "class LshTester():\n",
    "    # datas: datas for build hash index\n",
    "    # queries: query datas\n",
    "    # rand_range: random range for norm\n",
    "    # num_neighbours: query top num_neighbours\n",
    "    def __init__(self, datas, queries, rand_range = 1.0, num_neighbours = 1):\n",
    "        kdata, qdata = len(datas[0]), len(queries[0])\n",
    "\n",
    "        self.d = kdata\n",
    "        self.datas = datas\n",
    "        self.queries = queries\n",
    "        self.rand_range = rand_range\n",
    "        self.q_num = len(self.queries)\n",
    "        self.num_neighbours = num_neighbours\n",
    "\n",
    "    def linear(self, q, metric, max_results):\n",
    "        candidates = [(ix, metric(q, p)) for ix, p in enumerate(self.datas)]\n",
    "        temp = sorted(candidates, key=itemgetter(1))[:max_results]\n",
    "        return temp\n",
    "\n",
    "    def run(self, type, k_vec = [2], l_vec = [2]):\n",
    "        # set distance func object\n",
    "        if 'l2' == type:\n",
    "            metric = L2Lsh.distance\n",
    "\n",
    "        exact_hits = [[ix for ix, dist in self.linear(q, metric, self.num_neighbours)] for q in self.queries]   #exact_hits是距离q最近的那些点（这是真的，精准的）\n",
    "\n",
    "        print '=============================='\n",
    "        print type + ' TEST:'\n",
    "        print 'L\\tk\\tacc\\ttouch'\n",
    "\n",
    "        for k in k_vec:\n",
    "            lsh = LshWrapper(type, self.d, self.rand_range, k, 0)\n",
    "\n",
    "            for L in l_vec:\n",
    "                lsh.resize(L)\n",
    "                lsh.index(self.datas)\n",
    "\n",
    "                correct = 0\n",
    "                for q, hits in zip(self.queries, exact_hits):\n",
    "                    lsh_hits = [ix for ix, dist in lsh.query(q, metric, self.num_neighbours)]\n",
    "                    correct += int(lsh_hits == hits)\n",
    "\n",
    "                print \"{0}\\t{1}\\t{2}\\t{3}\".format(L, k, float(correct) / self.q_num, float(lsh.get_avg_touched()) / len(self.datas))\n",
    "\n",
    "class L2AlshTester(LshTester):\n",
    "    'L2-ALSH for MIPS Test parameters'\n",
    "\n",
    "    # datas: datas for build hash index\n",
    "    # queries: query datas\n",
    "    # rand_range: random range for norm\n",
    "    # num_neighbours: query top num_neighbours\n",
    "    # m: ALSH extend metrix length. default 3\n",
    "    def __init__(self, datas, queries, rand_range = 1.0, num_neighbours = 1, m = 3):\n",
    "        kdata = len(datas[0])\n",
    "        qdata = len(queries[0])\n",
    "        self.m = m\n",
    "        self.half_extend = [0.5 for i in xrange(self.m)]\n",
    "        # storage original datas & queries. used for validation\n",
    "        self.origin_datas = datas\n",
    "        self.origin_queries = queries\n",
    "        self.q_num = len(self.origin_queries)\n",
    "        # datas & queries transformation\n",
    "        dratio, dmax_norm, self.norm_datas = g_transformation(self.origin_datas)\n",
    "        self.norm_queries = g_normalization(self.origin_queries)\n",
    "        # expand k dimension into k+2m dimension\n",
    "        self.ext_datas = [(dv + g_ext_norm(dv, self.m) + [0.5 for i in xrange(self.m)]) for dv in self.norm_datas]\n",
    "        self.ext_queries = [(qv + [0.5 for i in xrange(self.m)] + g_ext_norm(qv, self.m)) for qv in self.norm_queries]\n",
    "        new_len = kdata + 2 * m\n",
    "        LshTester.__init__(self, self.ext_datas, self.ext_queries, rand_range, num_neighbours)\n",
    "\n",
    "    # MIPS\n",
    "    def linear(self, q, metric, max_results):\n",
    "        \"\"\" brute force search by linear scan \"\"\"\n",
    "        # print 'MipsLshTester linear:'\n",
    "        candidates = [(ix, np.dot(q, p)) for ix, p in enumerate(self.origin_datas)]\n",
    "        temp = sorted(candidates, key=itemgetter(1), reverse=True)[:max_results]\n",
    "        return temp\n",
    "\n",
    "    def run(self, type, k_vec = [2], l_vec = [2]):\n",
    "        validate_metric, compute_metric = np.dot, L2Lsh.distance\n",
    "        start = time.time()\n",
    "        exact_hits = [[ix for ix, dist in self.linear(q, compute_metric, self.num_neighbours)] for q in self.origin_queries]\n",
    "        exact_hits = [[ix for ix, dist in self.linear(q, validate_metric, self.num_neighbours)] for q in self.origin_queries]\n",
    "        linear_time = time.time() - start\n",
    "        print '=============================='\n",
    "        print 'L2AlshTester ' + type + ' TEST:'\n",
    "        print 'L\\tk\\tAcc\\tData_ratio\\tALSH time\\tLinear time'\n",
    "\n",
    "        # concatenating more hash functions increases selectivity\n",
    "        for k in k_vec:\n",
    "            lsh = LshWrapper(type, self.d, self.rand_range, k, 0)\n",
    "\n",
    "            # using more hash tables increases recall\n",
    "            for L in l_vec:\n",
    "                lsh.resize(L)\n",
    "                lsh.index(self.ext_datas)\n",
    "\n",
    "                correct = 0\n",
    "                start = time.time()\n",
    "                for q, hits in zip(self.ext_queries, exact_hits):\n",
    "                    lsh_hits = [ix for ix, dist in lsh.query(q, compute_metric, self.num_neighbours)]\n",
    "                    correct += int(lsh_hits == hits)\n",
    "                alsh_time = time.time() - start\n",
    "                print \"{0}\\t{1}\\t{2}\\t{3}\\t\\t{4}\\t\\t{5}\".format(L, k, float(correct) / self.q_num, float(lsh.get_avg_touched()) / len(self.datas), alsh_time, linear_time)\n",
    "\n",
    "    @staticmethod\n",
    "    def createTester(type, datas, queries, rand_num, num_neighbours):\n",
    "        return L2AlshTester(datas, queries, rand_num, num_neighbours)\n",
    "\n",
    "def lsh_test(datas, queries, rand_num, num_neighbours, mips = False):\n",
    "\n",
    "    type = 'l2'\n",
    "    tester = L2AlshTester.createTester(type, datas, queries, rand_num, num_neighbours)\n",
    "    args = {\n",
    "                'type':      type,\n",
    "                'k_vec':     [1, 2, 4, 8],\n",
    "                'l_vec':     [2, 4, 8, 16, 32]\n",
    "            }\n",
    "    tester.run(**args)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    type = 'cosine'\n",
    "    tester = LshTesterFactory.createTester(type, mips, datas, queries, rand_num, num_neighbours)\n",
    "\n",
    "    args = {\n",
    "                'type':      type,\n",
    "                'k_vec':    [1, 2, 4, 8],\n",
    "                'l_vec':    [2, 4, 8, 16, 32]\n",
    "            }\n",
    "    tester.run(**args)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     gol._init()\n",
    "    #gol.set_value('DEBUG', True)\n",
    "\n",
    "    # create a test dataset of vectors of non-negative integers\n",
    "\n",
    "\n",
    "    # Run this only after downloading the small or large dataset and make sure the path is set\n",
    "\n",
    "    # This is in case of small dataset\n",
    "    dataset = \"../datasets\" + os.path.sep + \"ml-latest-small\"\n",
    "\n",
    "    # This is in case of large dataset\n",
    "    # dataset = \"datasets\"+os.path.sep+\"ml-latest\"\n",
    "\n",
    "    # Name of the csv file\n",
    "    name = \"ratings.csv\"\n",
    "\n",
    "    # Read the csv file and get the appropriate column IDs\n",
    "    ratings_df = pd.read_csv(dataset + os.path.sep + name, names= [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], header=0)\n",
    "\n",
    "    # Converting to numbers and other changes\n",
    "    ratings_df[\"UserID\"] = pd.to_numeric(ratings_df[\"UserID\"], errors='ignore')\n",
    "    ratings_df[\"MovieID\"] = pd.to_numeric(ratings_df[\"MovieID\"], errors='ignore')\n",
    "    ratings_df[\"Rating\"] = pd.to_numeric(ratings_df[\"Rating\"], errors='ignore')\n",
    "    ratings_df[\"Timestamp\"] = pd.to_numeric(ratings_df[\"Timestamp\"], errors='ignore')\n",
    "\n",
    "\n",
    "    # Create the matrix of the rating of movies with the userIDs, fill with 0 for the missing values \n",
    "    R_df = ratings_df.pivot(index = 'UserID', columns ='MovieID', values = 'Rating').fillna(0)  \n",
    "\n",
    "    R = R_df.as_matrix()\n",
    "    user_ratings_mean = np.mean(R, axis = 1)\n",
    "    R_demeaned = R - user_ratings_mean.reshape(-1, 1)\n",
    "\n",
    "\n",
    "    # Divided the dataset into test(Query) and Train(Data Points)\n",
    "    #convert the rankings dataframe to sparse matrix\n",
    "    #     sR = sparse.csr_matrix(R_demeaned)\n",
    "    TRAIN_SIZE = 0.80\n",
    "\n",
    "    # Create boolean mask\n",
    "    # np.random creates a vector of random values between 0 and 1\n",
    "    # Those values are filtered to create a binary mask\n",
    "    #     msk = np.random.rand(sR.shape[0],sR.shape[1]) < TRAIN_SIZE\n",
    "    #     r = np.zeros(sR.shape)\n",
    "\n",
    "    train_ratings = R_demeaned[:100]\n",
    "    test_ratings = R_demeaned[100:120]\n",
    "    \n",
    "    # train_ratings = R_demeaned[:int(0.8*len(R_demeaned))]\n",
    "    # test_ratings = R_demeaned[int(0.8*len(R_demeaned)):]\n",
    "    #mask itself is random\n",
    "    #     train_ratings[msk] = r[msk]\n",
    "    #     test_ratings[~msk] = r[~msk] # inverse of boolean mask\n",
    "\n",
    "\n",
    "    test_r = np.array(test_ratings)\n",
    "    train_r = np.array(train_ratings)\n",
    "\n",
    "\n",
    "    num_neighbours = 1\n",
    "    radius = 0.3\n",
    "    r_range = 10 * radius\n",
    "\n",
    "\n",
    "    datas = train_r\n",
    "    queries = test_r\n",
    "    lsh_test(datas, queries, r_range, num_neighbours, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = \"../datasets\" + os.path.sep + \"ml-latest-small\"\n",
    "name = \"ratings.csv\"\n",
    "# Read the csv file and get the appropriate column IDs\n",
    "ratings_df = pd.read_csv(dataset + os.path.sep + name, names= [\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], header=0)\n",
    "\n",
    "# Converting to numbers and other changes\n",
    "ratings_df[\"UserID\"] = pd.to_numeric(ratings_df[\"UserID\"], errors='ignore')\n",
    "ratings_df[\"MovieID\"] = pd.to_numeric(ratings_df[\"MovieID\"], errors='ignore')\n",
    "ratings_df[\"Rating\"] = pd.to_numeric(ratings_df[\"Rating\"], errors='ignore')\n",
    "ratings_df[\"Timestamp\"] = pd.to_numeric(ratings_df[\"Timestamp\"], errors='ignore')\n",
    "\n",
    "\n",
    "# Create the matrix of the rating of movies with the userIDs, fill with 0 for the missing values \n",
    "R_df = ratings_df.pivot(index = 'UserID', columns ='MovieID', values = 'Rating').fillna(0)  \n",
    "R = R_df.as_matrix()\n",
    "user_ratings_mean = np.mean(R, axis = 1)\n",
    "R_demeaned = R - user_ratings_mean.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-2b287d6fd5b8>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-2b287d6fd5b8>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    test_r = =test_ratings.to_array()\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sR = sparse.csr_matrix(R_demeaned)\n",
    "TRAIN_SIZE = 0.80\n",
    "# Create boolean mask\n",
    "# np.random creates a vector of random values between 0 and 1\n",
    "# Those values are filtered to create a binary mask\n",
    "msk = np.random.rand(sR.shape[0],sR.shape[1]) < TRAIN_SIZE\n",
    "r = np.zeros(sR.shape)\n",
    "print (msk.shape)\n",
    " \n",
    "train_ratings = sR.copy()\n",
    "test_ratings = sR.copy()\n",
    "#mask itself is random\n",
    "train_ratings[msk] = r[msk]\n",
    "test_ratings[~msk] = r[~msk] # inverse of boolean mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671, 9066)\n",
      "(671, 9066)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(test_ratings))\n",
    "print(np.shape(train_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_r = test_ratings.toarray()\n",
    "train_r = train_ratings.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671, 9066)\n",
      "(671, 9066)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(test_r))\n",
    "print(np.shape(train_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_neighbours = 1\n",
    "r_global = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lsh_test(train_r, test_r, r_range, num_neighbours, True)\n",
    "L2AlshTester(train_r, test_r, r_range, num_neighbours)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
